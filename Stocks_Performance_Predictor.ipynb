{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOCKS PERFORMANCE PREDICTOR\n",
    "\n",
    "\n",
    "This notebook contains all the steps required to build a dataset of financial data for a lot of stocks and to analyze it with different machine learning models. The objective is to find stocks that will grow in value in the future. \n",
    "\n",
    "This notebook does not leverage historic ticker price data, but financial indicators found in the 10-K filings that each publicly traded company releases yearly.\n",
    "\n",
    "See the README.md for the background information. All packages used are easily retrievable and can be installed with either `pip` or `conda` depending on your Python setup. I used `Python 3.7.5`.\n",
    "\n",
    "An internet connection is required in order to scrape the data from the web (we will use https://financialmodelingprep.com and `pandas_datareader`).\n",
    "\n",
    "See tutorial folder for a guided example regarding the second part of this notebook.\n",
    "\n",
    "Let's begin from the required imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import json\n",
    "\n",
    "# Reading data from external sources\n",
    "import urllib as u\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Machine learning (preprocessing, models, evaluation)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Graphics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three helper functions that will be used throughout this notebook.\n",
    "\n",
    "`get_json_data`: used to scrape the links to financialmodelingprep API.\n",
    "\n",
    "`get_price_var`: used to compute the price variation during 2019, leverages `pandas_datareader` with Yahoo.\n",
    "\n",
    "`find_in_json`: used to scan a json file for a key and finding its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(url):\n",
    "    '''\n",
    "    Scrape data (which must be json format) from given url\n",
    "    Input: url to financialmodelingprep API\n",
    "    Output: json file\n",
    "    '''\n",
    "    response = urlopen(url)\n",
    "    dat = response.read().decode('utf-8')\n",
    "    return json.loads(dat)\n",
    "\n",
    "def get_price_var(symbol):\n",
    "    '''\n",
    "    Get historical price data for a given symbol leveraging the power of pandas_datareader and Yahoo.\n",
    "    Compute the difference between first and last available time-steps in terms of Adjusted Close price..\n",
    "    Input: ticker symbol\n",
    "    Output: price variation \n",
    "    '''\n",
    "    # read data\n",
    "    prices = data.DataReader(symbol, 'yahoo', '2019-01-01', '2019-12-31')['Adj Close']\n",
    "\n",
    "    # get all timestamps for specific lookups\n",
    "    today = prices.index[-1]\n",
    "    start = prices.index[0]\n",
    "\n",
    "    # calculate percentage price variation\n",
    "    price_var = ((prices[today] - prices[start]) / prices[start]) * 100\n",
    "    return price_var\n",
    "\n",
    "def find_in_json(obj, key):\n",
    "    '''\n",
    "    Scan the json file to find the value of the required key.\n",
    "    Input: json file\n",
    "           required key\n",
    "    Output: value corresponding to the required key\n",
    "    '''\n",
    "    # Initialize output as empty\n",
    "    arr = []\n",
    "\n",
    "    def extract(obj, arr, key):\n",
    "        '''\n",
    "        Recursively search for values of key in json file.\n",
    "        '''\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    extract(v, arr, key)\n",
    "                elif k == key:\n",
    "                    arr.append(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract(item, arr, key)\n",
    "        return arr\n",
    "\n",
    "    results = extract(obj, arr, key)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PART I: LIST OF STOCKS\n",
    "\n",
    "\n",
    "First, we need to get a list of stocks that will be used to build the dataset. Since there are thousands of stocks whose information can be scraped online, I decided to simply pull the whole list of available stocks on Financial Modeling Prep API. \n",
    "\n",
    "The list comprehends a total of more than 7k stocks, which clearly spans more than one sector. Indeed, each company belongs to its sector (Technology, Healthcare, Energy, ...), which in turn may be characterized by certain seasonalities, macro-economic trends and so on. As of now, I decided to focus on the Technology sector: this means that from the complete list of available stocks `available_tickers` I only keep those whose sector is equal to `Technology`. This operation is quite straight forward thanks to the power of `pandas` library.\n",
    "\n",
    "So, the list `tickers_tech` will contain all the available stocks, on Financial Modeling Prep API, belonging to the Technology sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://financialmodelingprep.com/api/v3/company/stock/list'\n",
    "ticks_json = get_json_data(url)\n",
    "available_tickers = find_in_json(ticks_json, 'symbol')\n",
    "\n",
    "tickers_sector = []\n",
    "for tick in tqdm(available_tickers):\n",
    "    url = 'https://financialmodelingprep.com/api/v3/company/profile/' + tick # get sector from here\n",
    "    a = get_json_data(url)\n",
    "    tickers_sector.append(find_in_json(a, 'sector'))\n",
    "\n",
    "S = pd.DataFrame(tickers_sector, index=available_tickers, columns=['Sector'])\n",
    "\n",
    "# Get list of tickers from TECHNOLOGY sector\n",
    "tickers_tech = S[S['Sector'] == 'Technology'].index.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PART II: GET PRICE VARIATION THROUGHOUT 2019\n",
    "\n",
    "\n",
    "The price variation of each stock listed in `tickers_tech` during 2019 will be used as metric to distinguish between stocks worth buying and those that are not. So, we need to:\n",
    "  * pull all the **daily Adjusted Close Price** for each stock, compute difference (this is done thanks to the helper function `get_price_var`\n",
    "  * if no data is found, skip the stock\n",
    "  * limit the number of stocks to be scanned to 1000\n",
    "  * store stocks and relative 2019 price variations in the dataframe `D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvar_list, tickers_found = [], []\n",
    "num_tickers_desired = 1000\n",
    "count = 0\n",
    "tot = 0\n",
    "TICKERS = tickers_tech\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    tot += 1 \n",
    "    try:\n",
    "        pvar = get_price_var(ticker)\n",
    "        pvar_list.append(pvar)\n",
    "        tickers_found.append(ticker)\n",
    "        count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stdout.write(f'\\rScanned {tot} tickers. Found {count}/{len(TICKERS)} usable tickers (max tickets = {num_tickers_desired}).')\n",
    "    stdout.flush()\n",
    "\n",
    "    if count == num_tickers_desired: # if there are more than 1000 tickers in sectors, stop\n",
    "        break\n",
    "\n",
    "# Store everything in a dataframe\n",
    "D = pd.DataFrame(pvar_list, index=tickers_found, columns=['2019 PRICE VAR [%]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the stocks in `D`, we now need to find the values of the indicators that will become the input data to the classification models. We leverage once again the FinancialModelingPrep API.\n",
    "\n",
    "First we load the `indicators.tx` file (available in the repository). As explained the the `README` document, a plethora of financial indicators are being scraped. I decided to perform a **brute force** of all the available indicators from the FinancialModelingPrep API, and then I will worry about cleaning and preparing the dataset for the models. The table below summarizes the quantity of financial indicator available for each category.\n",
    "\n",
    "\n",
    "||Income Statement|Balance Sheet Statement|Cash Flow Statement|Financial Ratios|Key Metrics|Financial Growth|\n",
    "|:-------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "|Quantity| 31 | 29 | 15 | 59 | 57 | 33 |\n",
    "\n",
    "\n",
    "In total, 224 indicators are available. However, since there are some duplicates, the actual number of indicators in `indicators.txt` is 221 (not counting the date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load financial indicators from the provided .txt file\n",
    "indicators = []\n",
    "filename = 'indicators.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        indicators.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PART III: SCRAPE FINANCIAL INDICATORS AND BUILD RAW DATASET\n",
    "\n",
    "\n",
    "As of now we have listed the stocks that belong to the Technology sector, and we have also listed their 2019 price variation. It is time to scrape the financial indicators that will later be used as input features to out classification models.\n",
    "\n",
    "The scraping will once again be performed thanks to the Financial Modeling Prep API. This process is quite time-consuming since it is required to pull a lot of data iteratively (due to a limit on the number of batch requests that can be performed with this API).\n",
    "\n",
    "Furthermore, it is important to keep in mind that:\n",
    "  * it is required to pull data within a specific time frame. Since the objective is the classification of a stock according to its price variation during year 2019, the financial indicators must belong to the end of year 2018. Historic financial data is an option that is considered as future development.\n",
    "  * it is possible, albeit uncommon, that one company filed two 10-K documents in the same year. In this case only the most recent entry must be kept.\n",
    "  * it is possible that the API does not return any data at all for a given stock. In this case the stock must be discarded.\n",
    "  * not all indicators will return a value. It is fair to expect that a percentage of the indicators are missing for one reason or another. In this case, `np.nan` will be assigned to the missing entries, and we will take care of them in the cleaning stage.\n",
    "\n",
    "In the end, what we want to obtain is a dataframe `DATA` where the rows correspond to the stocks for which the data has been found (`actual_tickers`) and the columns correspond to the financial indicators (`indicators`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists and dataframe (dataframe is a 2D numpy array filled with 0s)\n",
    "missing_tickers, missing_index = [], []\n",
    "d = np.zeros((len(tickers_found), len(indicators)))\n",
    "\n",
    "for t, _ in enumerate(tqdm(tickers_found)):\n",
    "    # Scrape indicators from financialmodelingprep API\n",
    "    url0 = 'https://financialmodelingprep.com/api/v3/financials/income-statement/' + tickers_found[t]\n",
    "    url1 = 'https://financialmodelingprep.com/api/v3/financials/balance-sheet-statement/' + tickers_found[t]\n",
    "    url2 = 'https://financialmodelingprep.com/api/v3/financials/cash-flow-statement/' + tickers_found[t]\n",
    "    url3 = 'https://financialmodelingprep.com/api/v3/financial-ratios/' + tickers_found[t]\n",
    "    url4 = 'https://financialmodelingprep.com/api/v3/company-key-metrics/' + tickers_found[t]\n",
    "    url5 = 'https://financialmodelingprep.com/api/v3/financial-statement-growth/' + tickers_found[t]\n",
    "    a0 = get_json_data(url0)\n",
    "    a1 = get_json_data(url1)\n",
    "    a2 = get_json_data(url2)\n",
    "    a3 = get_json_data(url3)\n",
    "    a4 = get_json_data(url4)\n",
    "    a5 = get_json_data(url5)\n",
    "    \n",
    "    # Combine all json files in a list, so that it can be scanned quickly\n",
    "    A = [a0, a1, a2 , a3, a4, a5]\n",
    "    all_dates = find_in_json(A, 'date')\n",
    "\n",
    "    check = [s for s in all_dates if '2018' in s] # find all 2018 entries in dates\n",
    "    if len(check) > 0:\n",
    "        date_index = all_dates.index(check[0]) # get most recent 2018 entries, if more are present\n",
    "\n",
    "        for i, _ in enumerate(indicators):\n",
    "            ind_list = find_in_json(A, indicators[i])\n",
    "            try:\n",
    "                d[t][i] = ind_list[date_index]\n",
    "            except:\n",
    "                d[t][i] = np.nan # in case there is no value inserted for the given indicator\n",
    "\n",
    "    else:\n",
    "        missing_tickers.append(tickers_found[t])\n",
    "        missing_index.append(t)\n",
    "\n",
    "actual_tickers = [x for x in tickers_found if x not in missing_tickers]\n",
    "d = np.delete(d, missing_index, 0)\n",
    "DATA = pd.DataFrame(d, index=actual_tickers, columns=indicators) # raw dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PART IV: DATASET CLEANING & PREPARATION\n",
    "\n",
    "\n",
    "The preparation of the dataset is somewhat an art form. I limited my actions to the application of the common practices, such as:\n",
    "\n",
    "  * removing columns that have a lot of `nan` values.\n",
    "  * removing columns that have a lot of `0` values.\n",
    "  * fill the remaining `nan` values with the average value of the column.\n",
    "\n",
    "For instance, in this specific case there are an average of 84 0-values per column, with a standard deviation of 140. So I decided to remove from the dataframe all those columns where the occurrences of 0-values is higher than 20 (20 being about 3.1% of the total number of rows of the dataset).\n",
    "\n",
    "At the same time, there is an average of about 37 `nan` entries per column, with a standard deviation of circa 86. So I decided to remove from the dataframe all those columns where the occurrences of `nan` entries is higher than 15 (15 being about 2.4% of the total number of rows of the dataset). Then, the remaining `nan` entries have been filled with the average value of the column.\n",
    "\n",
    "At the end of the cleaning process, the dataset `DATA` number of columns has decreased from 221 to 108, a 50% reduction. While certainly some of the discarded indicators were useless due to the lack of data, it is possible that useful data has been lost in this process. However, it must be considered that we need useful data across all stocks in the dataset, so I think it is acceptable to discard those indicators (columns) that may be relevant only to a subset of the dataset.\n",
    "\n",
    "\n",
    "Finally, it is required to classify each sample. For each stock it has already be computed the difference in trading price between the first trading day on January 2019 and the last trading day on December 2019 (dataset `D`). If this difference is positive, then that stock will belong to class `1`, which is a **BUY** signal. On the contrary, if the difference in price is negative, the stock will be classified as a `0`, which is an **IGNORE** signal (do not buy). A quick recap is found in the table below. \n",
    "\n",
    "\n",
    "| 2019 PRICE VARIATION | CLASS | SIGNAL |\n",
    "|:-----:|:-----:|:-----:|\n",
    "| >= 0  | 1 | BUY |\n",
    "| < 0   | 0 | IGNORE |\n",
    "\n",
    "\n",
    "So, this array of `1` and `0` values will be appended as the last column of the dataframe `DATA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that have more than 20 0-values\n",
    "DATA = DATA.loc[:, DATA1.isin([0]).sum() <= 20]\n",
    "\n",
    "# Remove columns that have more than 15 nan-values\n",
    "DATA = DATA.loc[:, DATA1.isna().sum() <= 15]\n",
    "\n",
    "# Fill remaining nan-values with column mean value\n",
    "DATA = DATA.apply(lambda x: x.fillna(x.mean())) \n",
    "\n",
    "# Get price variation data only for tickers to be used\n",
    "D2 = D.loc[DATA.index.values, :]\n",
    "\n",
    "# Generate classification array\n",
    "y = []\n",
    "for i, _ in enumerate(D2.index.values):\n",
    "    if D2.values[i] >= 0:\n",
    "        y.append(1)\n",
    "    else: \n",
    "        y.append(0)\n",
    "\n",
    "# Add array to dataframe\n",
    "DATA['class'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART I: DATASET SPLIT AND STANDARDIZATION\n",
    "\n",
    "\n",
    "In this second half of the notebook, the focus is shifted to the application of ML algorithms to classify the stocks collected into `DATA` in either BUY or IGNORE classes.\n",
    "\n",
    "As explained in the `README` file, 4 models are currently implemented:\n",
    "  * Support vector machine;\n",
    "  * Random forest;\n",
    "  * Extreme gradient boosting;\n",
    "  * Multi-layer perceptron (feed-forward neural network).\n",
    "  \n",
    "  \n",
    "Before training and testing them, it is required to perform a couple of pre-processing steps:\n",
    "1. split `DATA` in train and testing datasets;\n",
    "2. standardize each column of the dataset so that it has mean=0 and std=1.\n",
    "\n",
    "\n",
    "For each ML model, grid search with cross-validation is run in order to find the optimum set of the main hyper-parameters, with the objective of maximizing the accuracy.\n",
    "\n",
    "\n",
    "Finally, the models are compared with respect to the amount of $USD they'd return if a trader was to follow their predictions. The best model is the one that yields the highest gains.\n",
    "\n",
    "\n",
    "So, let's begin with splitting the dataset `DATA` and allocating the classification column `DATA['class']` to the array of ground truth `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data in train and testing\n",
    "train_split, test_split = train_test_split(DATA, test_size=0.2, random_state=1, stratify=df['class'])\n",
    "X_train = train_split.iloc[:, :-1].values\n",
    "y_train = train_split.iloc[:, -1].values\n",
    "X_test = test_split.iloc[:, :-1].values\n",
    "y_test = test_split.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `StandardScaler()` is applied in order to standardize each column of both training and testing datasets. It is important to use the same coefficients when standardizing both datasets, hence it is leveraged the `.fit_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize train/test datasets (use same coeff.)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART II: SVC\n",
    "\n",
    "\n",
    "The main hyper-parameters are optimized via `GridSearchCV` method (with 5 cross-validation folds). A set of possible parameters is defined in the dictionary `tuned_parameters`. Pay attention to the `scoring='precision_weighted'` parameter used with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "clf1 = GridSearchCV(SVC(random_state=1),\n",
    "                    tuned_parameters,\n",
    "                    n_jobs=6,\n",
    "                    scoring='precision_weighted',\n",
    "                    cv=5)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Best score, and parameters, found on development set:')\n",
    "print()\n",
    "print('%0.3f for %r' % (clf1.best_score_, clf1.best_params_))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART III: RFC\n",
    "\n",
    "\n",
    "The main hyper-parameters are optimized via `GridSearchCV` method (with 5 cross-validation folds). A set of possible parameters is defined in the dictionary `tuned_parameters`. Pay attention to the `scoring='precision_weighted'` parameter used with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {'n_estimators': [8, 32, 64, 128],\n",
    "                    'max_features': ['auto', 'sqrt'],\n",
    "                    'max_depth': [4, 5, 6, 7, 8],\n",
    "                    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "clf2 = GridSearchCV(RandomForestClassifier(random_state=1),\n",
    "                    tuned_parameters,\n",
    "                    n_jobs=6,\n",
    "                    scoring='precision_weighted',\n",
    "                    cv=5)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "print('Best score, and parameters, found on development set:')\n",
    "print()\n",
    "print('%0.3f for %r' % (clf2.best_score_, clf2.best_params_))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART IV: XGB\n",
    "\n",
    "\n",
    "The main hyper-parameters are optimized via `GridSearchCV` method (with 5 cross-validation folds). A set of possible parameters is defined in the dictionary `tuned_parameters`. Pay attention to the `scoring='precision_weighted'` parameter used with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {'learning_rate': [0.01, 0.001],\n",
    "              'max_depth': [4, 5, 6, 7, 8],\n",
    "              'n_estimators': [32, 256, 512, 1024]}\n",
    "\n",
    "clf3 = GridSearchCV(xgb.XGBClassifier(random_state=1),\n",
    "                   tuned_parameters,\n",
    "                   n_jobs=6,\n",
    "                   scoring='precision_weighted', \n",
    "                   cv=5)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "print('Best score, and parameters, found on development set:')\n",
    "print()\n",
    "print('%0.3f for %r' % (clf3.best_score_, clf3.best_params_))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART V: MLP\n",
    "\n",
    "\n",
    "The main hyper-parameters are optimized via `GridSearchCV` method (with 5 cross-validation folds). A set of possible parameters is defined in the dictionary `tuned_parameters`. Due to the relatively small amount of data, `batch_size = 4`. Pay attention to the `scoring='precision_weighted'` parameter used with `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {'hidden_layer_sizes': [(32,), (64,), (32, 64, 32)],\n",
    "                    'activation': ['tanh', 'relu'],\n",
    "                    'solver': ['lbfgs', 'adam']}\n",
    "\n",
    "clf4 = GridSearchCV(MLPClassifier(random_state=1, batch_size=4, early_stopping=True), \n",
    "                    tuned_parameters,\n",
    "                    n_jobs=6,\n",
    "                    scoring='precision_weighted',\n",
    "                    cv=5)\n",
    "\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "print('Best score, and parameters, found on development set:')\n",
    "print()\n",
    "print('%0.3f for %r' % (clf4.best_score_, clf4.best_params_))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MODELS PART VI: EVALUATION\n",
    "\n",
    "\n",
    "To evaluate the performance of the models, it is not enough to rank and compare their accuracies. Indeed, being a financial application, it is desired to maximize the profit. So, the models' predicted classes are gathered in a dataset `df1` and:\n",
    "\n",
    "  * if the predicted class = 1 --> Bob the trader buys $100 of that stock;\n",
    "  * if the predicted class = 0 --> Bob the trader ignores the stock.\n",
    "  \n",
    "\n",
    "Consequetly, we evaluate the price difference for each stock, exploiting the dataset `D` that has been built in the first half of the notebook, which collects the percent price variation of all stocks. The buy orders, price variations, start and final values are all collected in the dataframe `df1`.\n",
    "\n",
    "Since the dataframe `df1` is quite large, a more compact dataframe `MODELS_COMPARISON` is built in order to summarize the results so that it is easier to compare the performance of the ML models. The final row of `MODELS_COMPARISON` shows the net profit/loss for each ML model.\n",
    "\n",
    "As a reference, it is also reported the percent gain/loss of both S&P 500 (^GSPC) and DOW JONES (^DJI), which are usually considered as benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of stocks that belong to testing dataset\n",
    "tested_stocks = test_split.index.values\n",
    "\n",
    "# Get 2019 price variation ONLY of tested stocks\n",
    "df00 = D.loc[tested_stocks, :]\n",
    "\n",
    "# Initial investment can be $100 for each stock whose predicted class = 1\n",
    "buy_amount = 100\n",
    "\n",
    "# In new dataframe df1, store all the information regarding each model's predicted class and relative gain/loss in $USD\n",
    "df1 = pd.DataFrame(y_test, index=tested_stocks, columns=['ACTUAL']) # first column is the true class (BUY/INGORE)\n",
    "\n",
    "df1['SVM'] = clf1.predict(X_test) # predict class for testing dataset\n",
    "df1['VALUE START SVM [$]'] = df1['SVM'] * buy_amount # if class = 1 --> buy $100 of that stock\n",
    "df1['VAR SVM [$]'] = (df00['2019 PRICE VAR [%]'].values / 100) * df1['VALUE START SVM [$]'] # compute price variation in $\n",
    "df1['VALUE END SVM [$]'] = df1['VALUE START SVM [$]'] + df1['VAR SVM [$]'] # compute final value\n",
    "\n",
    "df1['RF'] = clf2.predict(X_test)\n",
    "df1['VALUE START RF [$]'] = df1['RF'] * buy_amount\n",
    "df1['VAR RF [$]'] = (df00['2019 PRICE VAR [%]'].values / 100) * df1['VALUE START RF [$]']\n",
    "df1['VALUE END RF [$]'] = df1['VALUE START RF [$]'] + df1['VAR RF [$]']\n",
    "\n",
    "df1['XGB'] = clf3.predict(X_test)\n",
    "df1['VALUE START XGB [$]'] = df1['XGB'] * buy_amount\n",
    "df1['VAR XGB [$]'] = (df00['2019 PRICE VAR [%]'].values / 100) * df1['VALUE START XGB [$]']\n",
    "df1['VALUE END XGB [$]'] = df1['VALUE START XGB [$]'] + df1['VAR XGB [$]']\n",
    "\n",
    "df1['MLP'] = clf4.predict(X_test)\n",
    "df1['VALUE START MLP [$]'] = df1['MLP'] * buy_amount\n",
    "df1['VAR MLP [$]'] = (df00['2019 PRICE VAR [%]'].values / 100) * df1['VALUE START MLP [$]']\n",
    "df1['VALUE END MLP [$]'] = df1['VALUE START MLP [$]'] + df1['VAR MLP [$]']\n",
    "\n",
    "# Create a new, compact, dataframe in order to show gain/loss for each model\n",
    "start_value_svm = df1['VALUE START SVM [$]'].sum()\n",
    "final_value_svm = df1['VALUE END SVM [$]'].sum()\n",
    "net_gain_svm = final_value_svm - start_value_svm\n",
    "percent_gain_svm = net_gain_svm / start_value_svm\n",
    "\n",
    "start_value_rf = df1['VALUE START RF [$]'].sum()\n",
    "final_value_rf = df1['VALUE END RF [$]'].sum()\n",
    "net_gain_rf = final_value_rf - start_value_rf\n",
    "percent_gain_rf = net_gain_rf / start_value_rf\n",
    "\n",
    "start_value_xgb = df1['VALUE START XGB [$]'].sum()\n",
    "final_value_xgb = df1['VALUE END XGB [$]'].sum()\n",
    "net_gain_xgb = final_value_xgb - start_value_xgb\n",
    "percent_gain_xgb = net_gain_xgb / start_value_xgb\n",
    "\n",
    "start_value_mlp = df1['VALUE START MLP [$]'].sum()\n",
    "final_value_mlp = df1['VALUE END MLP [$]'].sum()\n",
    "net_gain_mlp = final_value_mlp - start_value_mlp\n",
    "percent_gain_mlp = net_gain_mlp / start_value_mlp\n",
    "\n",
    "percent_gain_sp500 = get_price_var('^GSPC') # get percent gain of S&P500 index\n",
    "percent_gain_dj = get_price_var('^DJI') # get percent gain of DOW JONES index\n",
    "percent_gain_sector = D['2019 PRICE VAR [%]'].mean() # get percent gain of TECHNOLOGY sector\n",
    "\n",
    "MODELS_COMPARISON = pd.DataFrame([start_value_svm, final_value_svm, net_gain_svm, percent_gain_svm],\n",
    "                    index=['INITIAL COST [USD]', 'FINAL VALUE [USD]', 'GAIN/LOSS [USD]', 'GAIN/LOSS [%]'], columns=['SVM'])\n",
    "MODELS_COMPARISON['RF'] = [start_value_rf, final_value_rf, net_gain_rf, percent_gain_rf]\n",
    "MODELS_COMPARISON['XGB'] = [start_value_xgb, final_value_xgb, net_gain_xgb, percent_gain_xgb]\n",
    "MODELS_COMPARISON['MLP'] = [start_value_mlp, final_value_mlp, net_gain_mlp, percent_gain_mlp]\n",
    "MODELS_COMPARISON['S&P 500'] = ['', '', '', percent_gain_sp500]\n",
    "MODELS_COMPARISON['DOW JONES'] = ['', '', '', percent_gain_dj]\n",
    "MODELS_COMPARISON['SECTOR'] = ['', '', '', percent_gain_sector] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what concerns a more traditional comparison, it is also possible to print the `classification_report` for each ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(53 * '=')\n",
    "print(15 * ' ' + 'SUPPORT VECTOR MACHINE')\n",
    "print(53 * '-')\n",
    "print(classification_report(y_test, clf1.predict(X_test), target_names=['IGNORE', 'BUY']))\n",
    "print(53 * '-')\n",
    "print(53 * '=')\n",
    "print(20 * ' ' + 'RANDOM FOREST')\n",
    "print(53 * '-')\n",
    "print(classification_report(y_test, clf2.predict(X_test), target_names=['IGNORE', 'BUY']))\n",
    "print(53 * '-')\n",
    "print(53 * '=')\n",
    "print(14 * ' ' + 'EXTREME GRADIENT BOOSTING')\n",
    "print(53 * '-')\n",
    "print(classification_report(y_test, clf3.predict(X_test), target_names=['IGNORE', 'BUY']))\n",
    "print(53 * '-')\n",
    "print(53 * '=')\n",
    "print(15 * ' ' + 'MULTI-LAYER PERCEPTRON')\n",
    "print(53 * '-')\n",
    "print(classification_report(y_test, clf4.predict(X_test), target_names=['IGNORE', 'BUY']))\n",
    "print(53 * '-')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('FinPro': conda)",
   "language": "python",
   "name": "python37564bitfinprocondaa9ae4c24bd38409e9c51635336a4cf81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
